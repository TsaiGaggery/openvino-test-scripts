# âœ… VERIFIED Available OpenVINO Models (Nov 2025)

## ğŸ¯ Currently Enabled Models (All Verified âœ…)

These 7 models are **enabled** and **ready to benchmark**:

1. âœ… **TinyLlama-1.1B** (1.1B) - `OpenVINO/TinyLlama-1.1B-Chat-v1.0-int4-ov`
   - ğŸŸ¢ Excellent NPU performance
   - Fastest, most reliable
   - Last modified: 2024-10-31

2. âœ… **Qwen2.5-1.5B-Instruct** (1.5B) - `OpenVINO/Qwen2.5-1.5B-Instruct-int4-ov`
   - ğŸŸ¢ Great NPU performance
   - Latest Qwen generation
   - Last modified: 2025-04-28

3. âœ… **Phi-3-mini-128k** (3.8B) - `OpenVINO/Phi-3-mini-128k-instruct-int4-ov`
   - ğŸŸ¢ Good NPU performance
   - 128k context window
   - Last modified: 2024-10-31

4. âœ… **Phi-3.5-mini-instruct** (3.8B) - `OpenVINO/Phi-3.5-mini-instruct-int4-ov`
   - ğŸŸ¢ Good NPU performance
   - Updated Phi-3 version
   - Last modified: 2024-11-25

5. âœ… **Qwen3-4B** (4B) - `OpenVINO/Qwen3-4B-int4-ov`
   - ğŸŸ¡ May have NPU issues (test on CPU/GPU as fallback)
   - Good mid-size option
   - Last modified: 2025-05-30

6. âœ… **Mistral-7B-Instruct-v0.3** (7B) - `OpenVINO/Mistral-7B-Instruct-v0.3-int4-cw-ov`
   - ğŸ”µ Better on GPU
   - INT4-CW quantization
   - Last modified: 2025-07-08

7. âœ… **Qwen3-8B** (8B) - `OpenVINO/Qwen3-8B-int4-cw-ov`
   - ğŸ”µ Better on GPU
   - INT4-CW quantization
   - Last modified: 2025-07-09

---

## ğŸ“‹ Available But Disabled Models

These models **exist** but are currently **disabled**:

### Good Options to Enable:

1. âœ… **Mistral-7B-Instruct-v0.2** (7B) - `OpenVINO/Mistral-7B-Instruct-v0.2-int4-cw-ov`
   - Previous Mistral version
   - Good GPU performance

2. âœ… **Gemma-2B** (2B) - `OpenVINO/gemma-2b-it-int4-ov`
   - âš ï¸ Has NPU compilation issues (use GPU/CPU)
   - Last modified: 2024-11-25

### Known Issues:

3. âš ï¸ **Qwen3-1.7B** (1.7B) - `OpenVINO/Qwen3-1.7B-int4-ov`
   - âŒ NPU compilation error (use GPU/CPU only)
   - Last modified: 2025-04-30

---

## âŒ Models NOT Available (Don't Enable These)

These models **do not exist** on HuggingFace OpenVINO hub:

1. âŒ **Llama-2-7B-Chat** - `OpenVINO/llama-2-7b-chat-hf-int4-ov`
   - Model ID incorrect or removed

2. âŒ **SmolLM-135M** - `OpenVINO/SmolLM-135M-Instruct-int4-ov`
   - Not in OpenVINO hub

3. âŒ **StableLM-Zephyr-3B** - `OpenVINO/stablelm-zephyr-3b-int4-ov`
   - Not in OpenVINO hub

4. âŒ **Gemma-2-2B** - `OpenVINO/gemma-2-2b-it-int4-ov`
   - Not in OpenVINO hub

5. âŒ **Qwen2.5-3B-Instruct** - `OpenVINO/Qwen2.5-3B-Instruct-int4-ov`
   - Not released yet

6. âŒ **Llama-3.2-1B-Instruct** - `OpenVINO/Llama-3.2-1B-Instruct-int4-ov`
   - Not in OpenVINO hub yet

7. âŒ **Llama-3.2-3B-Instruct** - `OpenVINO/Llama-3.2-3B-Instruct-int4-ov`
   - Not in OpenVINO hub yet

---

## ğŸ¯ Recommended NPU Test Configuration

### Quick NPU Test (3 models - all verified):
```json
{
  "TinyLlama-1.1B": true,
  "Qwen2.5-1.5B-Instruct": true,
  "Phi-3-mini-128k": true
}
```

### Comprehensive NPU Test (5 models):
```json
{
  "TinyLlama-1.1B": true,
  "Qwen2.5-1.5B-Instruct": true,
  "Phi-3-mini-128k": true,
  "Phi-3.5-mini-instruct": true,
  "Qwen3-4B": true
}
```

### Multi-Size Comparison (7 models - all sizes):
```json
{
  "TinyLlama-1.1B": true,           // 1B - NPU
  "Qwen2.5-1.5B-Instruct": true,    // 1.5B - NPU
  "Phi-3-mini-128k": true,          // 3.8B - NPU
  "Phi-3.5-mini-instruct": true,    // 3.8B - NPU
  "Qwen3-4B": true,                 // 4B - NPU/GPU
  "Mistral-7B-Instruct-v0.3": true, // 7B - GPU
  "Qwen3-8B": true                  // 8B - GPU
}
```

---

## ğŸ” How to Verify Before Running

Always run the verification script before benchmarking:

```bash
python3 verify_models.py
```

This will:
- âœ… Check all models in benchmark.json
- âœ… Show which are available/not available
- âœ… Warn about enabled models that don't exist
- âœ… Give you a summary of what's ready to benchmark

---

## ğŸ“Š Performance Expectations

### NPU Performance (for available models):

| Model | Size | Expected Speed | Recommendation |
|-------|------|----------------|----------------|
| TinyLlama-1.1B | 1.1B | 30-40 tok/s | â­ Best NPU |
| Qwen2.5-1.5B | 1.5B | 25-35 tok/s | â­ Great NPU |
| Phi-3-mini | 3.8B | 15-22 tok/s | âœ… Good NPU |
| Phi-3.5-mini | 3.8B | 15-22 tok/s | âœ… Good NPU |
| Qwen3-4B | 4B | 12-18 tok/s | âš ï¸ May have issues |
| Mistral-7B | 7B | 8-12 tok/s | ğŸ”µ Use GPU |
| Qwen3-8B | 8B | 6-10 tok/s | ğŸ”µ Use GPU |

---

## ğŸ› ï¸ If You Need More Models

### Search OpenVINO Hub:
Visit: https://huggingface.co/OpenVINO

### Look for:
- Models ending in `-int4-ov` or `-int4-cw-ov`
- Recently updated (2024-2025)
- Text generation task

### Add to benchmark.json:
1. Copy the exact model ID from HuggingFace
2. Add to `benchmark.json`
3. Run `python3 verify_models.py` to confirm
4. Enable and test!

---

## âœ… Current Status Summary

- **Total models in config**: 17
- **Actually available**: 10 (59%)
- **Enabled and available**: 7 (100% ready! âœ…)
- **Ready to benchmark**: YES! ğŸš€

All enabled models have been verified and are ready to run!

```bash
# You're ready to run:
python benchmark_devices.py
```

The benchmark will now:
- âœ… Download only models that exist
- âœ… Skip problematic NPU models automatically  
- âœ… Continue on errors
- âœ… Generate beautiful HTML report
- âœ… Complete successfully!
