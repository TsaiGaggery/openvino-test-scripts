{
  "models": [
  ],
  "benchmark_config": {
    "test_prompts": [
      "Why Intel established OpenVino, why didn't they use existing framework to run AI models?",
      "If you are the president of the US, how you resolve the healthcare system in the US?",
      "Do you think EV car is the future? Will it replace gas car?",
      "What is the most popular sports in the US?",
      "Why NPU loading time is so long?"
    ],
    "generation_config": {
      "max_new_tokens": 5000,
      "temperature": 0.7,
      "top_p": 0.9,
      "do_sample": true
    },
    "devices_to_test": ["CPU", "GPU", "NPU"],
    "run_warmup": true,
    "cache_dir": "./ov_cache"
  },
  "notes": {
    "how_to_enable": "Set 'enabled': true for models you want to benchmark",
    "recommended_start": "Start with 1-2 small models (1-4B) to test quickly",
    "npu_optimization": "Models with 'NPU' in recommended_devices are optimized for Intel NPU",
    "size_guide": "1-2B: Fastest on NPU, 3-5B: Good balance, 7-8B: Best quality but slower",
    "model_id_note": "All model_ids use 'OpenVINO/' prefix - these are pre-optimized INT4 models from HuggingFace Hub. If a model is not available, check https://huggingface.co/OpenVINO for the correct model ID.",
    "performance_tips": {
      "first_run": "First run will be slow due to compilation - subsequent runs use cache and are much faster",
      "cache_location": "Cache is stored in ./ov_cache directory",
      "npu_sweet_spot": "For best NPU performance, use models between 1-4B parameters",
      "quality_vs_speed": "Larger models have better quality but slower speed. For NPU, staying under 5B gives best speed/quality balance"
    }
  }
}
